{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4bdaefcb-be75-4ce0-8daa-262e1d440902",
   "metadata": {},
   "source": [
    "# Training a Convolutional Neural Network for digit recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad0be502-fa63-4ac1-a46d-317ce46c186f",
   "metadata": {},
   "source": [
    "This was built following a tutorial in the wonderful book *Machine Learning with PyTorch and Scikit-Learn* by Raschka, Liu and Mirjalili (2022, Packt Publishing).  In this notebook, we do the following:\n",
    "\n",
    "1. Use the PyTorch library (https://pytorch.org/) to construct and train a convolutional neural network (CNN) on the MNIST handwritten digit database (http://yann.lecun.com/exdb/mnist/).\n",
    "2. Deploy the trained model as an interactive web app using the Gradio library (https://gradio.app/).\n",
    "\n",
    "Note that the Gradio app can be found by visiting my huggingface page: https://huggingface.co/spaces/etweedy/digits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "291a59f1-ec20-4f43-a5c9-61478ee9a109",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32d27b15-ce93-40b4-bb41-e42bd9b19cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import Subset, DataLoader\n",
    "image_path = 'data'\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c27c48b-a37f-45b5-884c-0b8c82017398",
   "metadata": {},
   "source": [
    "## Load and the MNIST dataset and create PyTorch DataLoaders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d08677b5-14ea-448f-81a9-dc18c4833ce9",
   "metadata": {},
   "source": [
    "We first load the datasets and create training and validation DataLoaders with batch_size=64 samples.\n",
    "Some remarks:\n",
    "1. The MNIST images are black and white (i.e. 1 channel) 28x28 pixel images files.  By default, datasets.MNIST() loads in each sample as a tuple (image,label) where image is is a PIL image and label is the ground truth value of the digit.\n",
    "2. The ToTensor() transform does two things in succession:\n",
    "    - Converts the image in each tuple to a PyTorch tensor, sending each pixel to a float between 0 and 255.\n",
    "    - Normalizes the floats to in the tensor to the interval [0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7ff02963-841d-46fa-a316-add00df9b26a",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "74c45a47-87ea-46b0-81df-dfeb17cdd765",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = datasets.MNIST(root='data',transform = transform,train=True,download=True)\n",
    "data_val = datasets.MNIST(root='data',transform = transform, train=False, download=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4b802345-d289-410f-8642-489cf8236044",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=64\n",
    "torch.manual_seed(1)\n",
    "dl_train = DataLoader(data_train,batch_size,shuffle=True)\n",
    "dl_val = DataLoader(data_val,batch_size,shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "951b598e-a443-4ab8-9aa1-5494dc9d96cc",
   "metadata": {},
   "source": [
    "## Construct CNN model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b8d87b3-a443-4aa3-8529-3a731332bcea",
   "metadata": {},
   "source": [
    "We build a CNN with two convolutive layers with batch normalization, ReLU activation, and 2x2 max-pooling, followed by a flattening layer and two linear layers with a dropout layer between."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a9dc68cd-0b5b-47bd-8102-9f824832e1f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (conv1): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "  (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu1): ReLU()\n",
       "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv2): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "  (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu2): ReLU()\n",
       "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (fc1): Linear(in_features=3136, out_features=1024, bias=True)\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       "  (fc2): Linear(in_features=1024, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = nn.Sequential()\n",
    "model.add_module(\n",
    "    'conv1',\n",
    "    nn.Conv2d(\n",
    "        in_channels=1,out_channels=32,\n",
    "        kernel_size=5,padding=2\n",
    "    ),\n",
    ")\n",
    "model.add_module('bn1',nn.BatchNorm2d(32))\n",
    "model.add_module('relu1',nn.ReLU())\n",
    "model.add_module('pool1',nn.MaxPool2d(kernel_size=2))\n",
    "model.add_module(\n",
    "    'conv2',\n",
    "    nn.Conv2d(\n",
    "        in_channels=32,out_channels=64,\n",
    "        kernel_size=5,padding=2\n",
    "    ),\n",
    ")\n",
    "model.add_module('bn2',nn.BatchNorm2d(64))\n",
    "model.add_module('relu2',nn.ReLU())\n",
    "model.add_module('pool2',nn.MaxPool2d(kernel_size=2))\n",
    "model.add_module('flatten',nn.Flatten())\n",
    "model.add_module('fc1',nn.Linear(3136,1024))\n",
    "model.add_module('dropout',nn.Dropout(p=0.5))\n",
    "model.add_module('fc2',nn.Linear(1024,10))\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a771b2f9-baf6-4669-8d05-4947c9b6135b",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "opt = optim.Adam(model.parameters(),lr=0.002)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9552302c-c550-466f-bac0-b053673483ee",
   "metadata": {},
   "source": [
    "## Training the CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fda55a5-abcd-4941-b5b3-791868908096",
   "metadata": {},
   "source": [
    "We now construct our training function, which keeps track of training loss and accuracy and validation loss and accuracy after each epoch.  Accuracy values are printed as we progress through training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "62227236-2763-45f7-8ab4-4c0e00477df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model,num_epochs,dl_train,dl_val):\n",
    "    loss_hist_train = [0]*num_epochs\n",
    "    acc_hist_train = [0]*num_epochs\n",
    "    loss_hist_val = [0]*num_epochs\n",
    "    acc_hist_val = [0]*num_epochs\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for x_batch,y_batch in dl_train:\n",
    "            x_batch=x_batch.to(device)\n",
    "            y_batch=y_batch.to(device)\n",
    "            pred = model(x_batch)\n",
    "            loss = loss_fn(pred,y_batch)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            opt.zero_grad()\n",
    "            loss_hist_train[epoch] += loss.item()*y_batch.size(0)\n",
    "            is_correct=(torch.argmax(pred,dim=1) == y_batch).float()\n",
    "            acc_hist_train[epoch] += is_correct.sum()\n",
    "            \n",
    "        loss_hist_train[epoch] /= len(dl_train.dataset)\n",
    "        acc_hist_train[epoch] /= len(dl_train.dataset)\n",
    "        \n",
    "        model.eval()\n",
    "    \n",
    "        with torch.no_grad():\n",
    "            for x_batch,y_batch in dl_val:\n",
    "                x_batch=x_batch.to(device)\n",
    "                y_batch=y_batch.to(device)\n",
    "                pred = model(x_batch)\n",
    "                loss = loss_fn(pred,y_batch)\n",
    "                loss_hist_val[epoch] += loss.item()*y_batch.size(0)\n",
    "                is_correct=(torch.argmax(pred,dim=1) == y_batch).float()\n",
    "                acc_hist_val[epoch] += is_correct.sum()\n",
    "            loss_hist_val[epoch] /= len(dl_val.dataset)\n",
    "            acc_hist_val[epoch] /= len(dl_val.dataset)\n",
    "        \n",
    "            print(f' Epoch {epoch+1} ---- train accuracy: {acc_hist_train[epoch]:.4f} ---- val accuracy: {acc_hist_val[epoch]:.4f}')\n",
    "        \n",
    "    return loss_hist_train,loss_hist_val,acc_hist_train,acc_hist_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "b2c059fd-6ad4-4003-86fc-f4bf62dfd282",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1)\n",
    "num_epochs=20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a90983-87b1-40f0-a932-3532849a678d",
   "metadata": {},
   "source": [
    "After 20 epochs, we are able to reach a validation accuracy of 99.18%!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "7fec6ef3-6c17-48de-b366-dc2efb368516",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch 1 ---- train accuracy: 0.9371 ---- val accuracy: 0.9852\n",
      " Epoch 2 ---- train accuracy: 0.9832 ---- val accuracy: 0.9870\n",
      " Epoch 3 ---- train accuracy: 0.9864 ---- val accuracy: 0.9916\n",
      " Epoch 4 ---- train accuracy: 0.9895 ---- val accuracy: 0.9823\n",
      " Epoch 5 ---- train accuracy: 0.9894 ---- val accuracy: 0.9894\n",
      " Epoch 6 ---- train accuracy: 0.9903 ---- val accuracy: 0.9876\n",
      " Epoch 7 ---- train accuracy: 0.9901 ---- val accuracy: 0.9884\n",
      " Epoch 8 ---- train accuracy: 0.9905 ---- val accuracy: 0.9914\n",
      " Epoch 9 ---- train accuracy: 0.9919 ---- val accuracy: 0.9904\n",
      " Epoch 10 ---- train accuracy: 0.9920 ---- val accuracy: 0.9919\n",
      " Epoch 11 ---- train accuracy: 0.9931 ---- val accuracy: 0.9895\n",
      " Epoch 12 ---- train accuracy: 0.9930 ---- val accuracy: 0.9895\n",
      " Epoch 13 ---- train accuracy: 0.9934 ---- val accuracy: 0.9879\n",
      " Epoch 14 ---- train accuracy: 0.9937 ---- val accuracy: 0.9896\n",
      " Epoch 15 ---- train accuracy: 0.9946 ---- val accuracy: 0.9903\n",
      " Epoch 16 ---- train accuracy: 0.9944 ---- val accuracy: 0.9898\n",
      " Epoch 17 ---- train accuracy: 0.9947 ---- val accuracy: 0.9892\n",
      " Epoch 18 ---- train accuracy: 0.9945 ---- val accuracy: 0.9906\n",
      " Epoch 19 ---- train accuracy: 0.9954 ---- val accuracy: 0.9899\n",
      " Epoch 20 ---- train accuracy: 0.9961 ---- val accuracy: 0.9918\n"
     ]
    }
   ],
   "source": [
    "hist = train(model,num_epochs,dl_train,dl_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d32a322-0ac6-4b60-a3db-bb6407b2033e",
   "metadata": {},
   "source": [
    "## Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "4c465c01-72b4-4fd5-ae2c-a699296d4e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model,'mnist_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "112879c0-8510-4fb7-97cf-8ce1947b78fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(),'mnist_model_weights.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed2d25e-bbde-417d-9dd7-071bb8618379",
   "metadata": {},
   "source": [
    "## Gradio app implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee4821b-8048-465c-985a-5606e9d9af88",
   "metadata": {},
   "source": [
    "The following code creates an interactive Gradio app, which will ask the user to draw a digit on an in-browser sketchpad and then guess the digit using the model we've trained.  See this link for an implementation hosted on my huggingface account: https://huggingface.co/spaces/etweedy/digits\n",
    "\n",
    "There are several steps to this implementation:\n",
    "1. Write a prediction function which will take in an image from the Gradio sketchpad and make a prediction of the digit using our model.\n",
    "2. Write the code that launchest the Gradio interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5505f3-ece5-49a9-bea6-c3dfb4d19572",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(img):\n",
    "    x = torch.tensor(img, dtype=torch.float32).unsqueeze(0).unsqueeze(0) / 255.\n",
    "    with torch.no_grad():\n",
    "        pred = model(x)[0]\n",
    "    return int(pred.argmax())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d42b7e-a7dd-43ef-9345-9ef9b5d30717",
   "metadata": {},
   "source": [
    "Note that if you're running this notebook on the cloud (Google Collab, Kaggle, Paperspace, etc.) the link created by the below code may not work.  It creates a locally hosted version of your web app which you can open and play with in your browser, if you are running this notebook on your local machine.\n",
    "\n",
    "It's easy to share your machine learning project as a Gradio space on huggingface! More info: https://huggingface.co/blog/gradio-spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ade51b-b749-4b5b-ae37-fc2879ad763a",
   "metadata": {},
   "outputs": [],
   "source": [
    "title = \"Guess that digit\"\n",
    "description = \"Draw your favorite base-10 digit (0-9) and click submit - I'll try to guess what you drew! I do a bit better if you're not too messy and your digit is fairly centered.\"\n",
    "gr.Interface(fn=predict, \n",
    "             inputs=\"sketchpad\",\n",
    "             outputs=\"label\",\n",
    "             title = title,\n",
    "             description = description,\n",
    "              ).launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
